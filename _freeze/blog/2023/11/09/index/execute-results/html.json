{
  "hash": "05eae46e9848d0a01324286dbb67c84d",
  "result": {
    "markdown": "---\ntitle: \"Support Vector Machine\"\ndate: 2023-11-09\ndate-modified: last-modified\ncategories:\n  - r\n  - svm\nimage: svm.png\ndescription: SVM takes groups of observations and construct boundaries to predict which group future observations belong to based on their measurements.\n---\n\n\n\n\nThis context credited from [here](https://uc-r.github.io/svm)\n\n## Load package\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Set pseudorandom number generator\nset.seed(1233)\n\n### Load packages\nlibrary(here)         # relative path\n## here() starts at /Users/zhonggr/Library/CloudStorage/OneDrive-Personal/quarto\nlibrary(tidyverse)    # data manipulation and visualization\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nlibrary(kernlab)      # SVM methodology\n## \n## Attaching package: 'kernlab'\n## \n## The following object is masked from 'package:purrr':\n## \n##     cross\n## \n## The following object is masked from 'package:ggplot2':\n## \n##     alpha\nlibrary(e1071)        # SVM methodology\nlibrary(ISLR)         # contains example dataset \"Khan\"\nlibrary(RColorBrewer) # customized coloring of plots\n```\n:::\n\n\n## Maximal margin classifier\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Construct sample data set- completely separated\nx <- matrix(rnorm(20*2), ncol = 2)\ny <- c(rep(-1, 10), rep(1, 10))\nx[y==1, ] <- x[y == 1, ] + 3/2\ndat  <- data.frame(x = x, y = as.factor(y))\n\n### Plot data\nggplot(dat, aes(x = x.2, y = x.1, color = y, shape = y)) +\n    geom_point(size = 2) +\n    scale_color_manual(values = c(\"#000000\", \"#FF0000\")) +\n    theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n### Fit svm to data set\nsvmfit <- svm(y ~., data = dat, kernel = \"linear\", scale = FALSE)\n\n### Plot results\nplot(svmfit, dat)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Fit model and produce plot\nkernfit <- ksvm(x, y, type = \"C-svc\", kernel = \"vanilladot\")\n##  Setting default kernel parameters\nplot(kernfit, data = x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Support Vector Classifiers\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Construct sample data set - not completely separated\nx <- matrix(rnorm(20*2), ncol = 2)\ny <- c(rep(-1,10), rep(1,10))\nx[y==1,] <- x[y==1,] + 1\ndat <- data.frame(x=x, y=as.factor(y))\n\n### Plot data\nggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + \n  geom_point(size = 2) +\n  scale_color_manual(values=c(\"#000000\", \"#FF0000\")) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Fit Support Vector Machine model to data set\nsvmfit <- svm(y~., data = dat, kernel = \"linear\", cost = 10)\n# Plot Results\nplot(svmfit, dat)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Fit Support Vector Machine model to data set\nkernfit <- ksvm(x,y, type = \"C-svc\", kernel = 'vanilladot', C = 100)\n##  Setting default kernel parameters\n# Plot results\nplot(kernfit, data = x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Find optimal cost of misclassification\ntune_out <- tune(svm, y~., data = dat, kernel = \"linear\",\n                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n\n### Extract the best model\nbestmod <- tune_out$best.model\nbestmod\n## \n## Call:\n## best.tune(METHOD = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, \n##     0.01, 0.1, 1, 5, 10, 100)), kernel = \"linear\")\n## \n## \n## Parameters:\n##    SVM-Type:  C-classification \n##  SVM-Kernel:  linear \n##        cost:  5 \n## \n## Number of Support Vectors:  15\n\n### Create a table of misclassified observations\nypred <- predict(bestmod, dat)\n\nmisclass <- table(predict = ypred, truth = dat$y)\nmisclass\n##        truth\n## predict -1 1\n##      -1  7 3\n##      1   3 7\n```\n:::\n\n\n## Support Vector Machines\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Construct larger random data set\nx <- matrix(rnorm(200*2), ncol = 2)\nx[1:100,] <- x[1:100,] + 2.5\nx[101:150,] <- x[101:150,] - 2.5\ny <- c(rep(1,150), rep(2,50))\ndat <- data.frame(x=x,y=as.factor(y))\n\n# Plot data\nggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + \n  geom_point(size = 2) +\n  scale_color_manual(values=c(\"#000000\", \"#FF0000\")) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Set pseudorandom number generator\nset.seed(123)\n\n### Sample training data and fit model\ntrain <- base::sample(200,100, replace = FALSE)\nsvmfit <- svm(y~., data = dat[train,], kernel = \"radial\", gamma = 1, cost = 1)\n\n# Plot classifier\nplot(svmfit, dat)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fit radial-based SVM in kernlab\nkernfit <- ksvm(x[train,],y[train], type = \"C-svc\", kernel = 'rbfdot', C = 1, scaled = c())\n\n# Plot training data\nplot(kernfit, data = x[train,])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Tune model to find optimal cost, gamma values\ntune_out <- tune(svm, y~., data = dat[train,], kernel = \"radial\",\n                 ranges = list(cost = c(0.1,1,10,100,1000),\n                 gamma = c(0.5,1,2,3,4)))\n### Show best model\ntune_out$best.model\n## \n## Call:\n## best.tune(METHOD = svm, train.x = y ~ ., data = dat[train, ], ranges = list(cost = c(0.1, \n##     1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)), kernel = \"radial\")\n## \n## \n## Parameters:\n##    SVM-Type:  C-classification \n##  SVM-Kernel:  radial \n##        cost:  0.1 \n## \n## Number of Support Vectors:  62\n\n### Validate model performance\nvalid <- table(\n    true = dat[-train,\"y\"], \n    pred = predict(tune_out$best.model, newx = dat[-train,])\n)\nvalid\n##     pred\n## true  1  2\n##    1 53 30\n##    2 12  5\n```\n:::\n\n\n## SVMs for Multiple Classess\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Construct data set\nx <- rbind(x, matrix(rnorm(50*2), ncol = 2))\ny <- c(y, rep(0,50))\nx[y==0,2] <- x[y==0,2] + 2.5\ndat <- data.frame(x=x, y=as.factor(y))\n\n### Plot data set\nggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + \n  geom_point(size = 2) +\n  scale_color_manual(values=c(\"#000000\",\"#FF0000\",\"#00BA00\")) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Fit model\nsvmfit <- svm(y~., data = dat, kernel = \"radial\", cost = 10, gamma = 1)\n### Plot results\nplot(svmfit, dat)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Construct table\nypred <- predict(svmfit, dat)\nmisclass <- table(predict = ypred, truth = dat$y)\nmisclass\n##        truth\n## predict   0   1   2\n##       0  37   4   2\n##       1   8 140   3\n##       2   5   6  45\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Fit and plot\nkernfit <- ksvm(\n    as.matrix(dat[,2:1]),dat$y, type = \"C-svc\", \n    kernel = 'rbfdot', C = 100, scaled = c()\n)\n\n### Create a fine grid of the feature space\nx.1 <- seq(from = min(dat$x.1), to = max(dat$x.1), length = 100)\nx.2 <- seq(from = min(dat$x.2), to = max(dat$x.2), length = 100)\nx.grid <- expand.grid(x.2, x.1)\n\n### Get class predictions over grid\npred <- predict(kernfit, newdata = x.grid)\n\n### Plot the results\ncols <- brewer.pal(3, \"Set1\")\nplot(x.grid, pch = 19, col = adjustcolor(cols[pred], alpha.f = 0.05))\n\nclasses <- matrix(pred, nrow = 100, ncol = 100)\ncontour(\n    x = x.2, y = x.1, z = classes, levels = 1:3, \n    labels = \"\", add = TRUE\n)\n\npoints(dat[, 2:1], pch = 19, col = cols[predict(kernfit)])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n## Application\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Fit model\ndat <- data.frame(x = Khan$xtrain, y=as.factor(Khan$ytrain))\nout <- svm(y~., data = dat, kernel = \"linear\", cost=10)\nout\n## \n## Call:\n## svm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 10)\n## \n## \n## Parameters:\n##    SVM-Type:  C-classification \n##  SVM-Kernel:  linear \n##        cost:  10 \n## \n## Number of Support Vectors:  58\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### Check model performance on training set\ntable(out$fitted, dat$y)\n##    \n##      1  2  3  4\n##   1  8  0  0  0\n##   2  0 23  0  0\n##   3  0  0 12  0\n##   4  0  0  0 20\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n### validate model performance\ndat.te <- data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))\npred.te <- predict(out, newdata=dat.te)\ntable(pred.te, dat.te$y)\n##        \n## pred.te 1 2 3 4\n##       1 3 0 0 0\n##       2 0 6 2 0\n##       3 0 0 4 0\n##       4 0 0 0 5\n```\n:::\n\n\n## Reference\n\n- [Support Vector Machine](https://uc-r.github.io/svm)\n- [Support Vector Machines in R](https://www.datacamp.com/tutorial/support-vector-machines-r)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}